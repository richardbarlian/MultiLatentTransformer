# ğŸ§  Tiny Multi-Latent Transformer

> A self-driven exploration into generative modeling, inspired by the _Multi-Latent Transformer_ paper and the internals of GPT-style transformers.  
> Built entirely from scratch using **PyTorch** and **PyTorch Lightning**.

---

## ğŸš€ Overview

This project is a **hobby deep learning initiative** where I implemented and trained a decoder-only transformer with **modern research-backed improvements**. Inspired by the [_TransMLA: Multi-Head Latent Attention Is All You Need_ (2025)](https://arxiv.org/abs/2502.07864), it serves as a minimal and modular playground for token-level language modeling and advanced attention designs.

The project was born out of a desire to truly understand AI systems â€” not by using black-box APIs, but by reconstructing them from first principles.

---

## ğŸ“Œ Key Features & Contributions

### âœ… Research-Inspired Design

- Built around the **Multi-Latent Transformer** concept â€” enabling multiple latent attention pathways for parallel reasoning.
- Closely follows the "Attention is All You Need" paper, but adopts a **post-LayerNorm** design for better stability.

### ğŸ”§ Engineering Enhancements I Added

- âš¡ï¸ **Memory-efficient data loading** via `PackedDataset` with overlapping (stride-based) token sequences.
- ğŸ§  **Cosine annealing learning rate schedule** with linear warm-up, implemented using PyTorch's `LambdaLR`.
- ğŸ§© **GPT-style parameter initialization** and grouped weight decay for stable convergence.
- ğŸ“ˆ Integrated with **TensorBoard** for simple, real-time logging of training curves and metrics.
- ğŸ§ª **Validation perplexity evaluation** at regular intervals and end of training.
- ğŸ›‘ **Early stopping and model checkpointing** for fail-safe experiments.
- ğŸ•¸ï¸ Created `scraper.py` to download and preprocess public domain books from **Project Gutenberg**.

---

## ğŸ› ï¸ Tech Stack

- `PyTorch` + `PyTorch Lightning`
- `tiktoken` for GPT-2 style BPE tokenization
- Mixed-precision (`fp16`) training
- TensorBoard + Rich CLI logging
- Modular config-driven experiment management

---

## ğŸ“Š Training Setup

| Setting             | Value           |
| ------------------- | --------------- |
| Block Size          | 512 tokens      |
| Batch Size          | 20              |
| Max Iterations      | 18,370          |
| Learning Rate       | 3e-4            |
| Min Learning Rate   | 3e-5            |
| Warmup Steps        | 10% of total    |
| Validation Interval | every 500 steps |
| Gradient Clipping   | 1.0             |
| Accumulated Batches | 4               |
| Final Metric        | Perplexity      |

---

## â–¶ï¸ How to Use

```bash
# Clone the repo and install dependencies
pip install -r requirements.txt

# Download dataset from Project Gutenberg
python scraper.py

# Train the model
python train.py
```
